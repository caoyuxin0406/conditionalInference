#' Group Lasso Model with Instrumental Variables for AR Statistic
#'
#' Initialize outcome, treatment, and instrument matrices as well as
#' other terms for penalized convex optimization program.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#' @param penalty Lambda term in equation (13) and (15).
#' @param ridge_term L2 regularization term (if applicable). Default to 0.
#' @param randomizer_scale Standard deviation of random noise distribution.
#' @param perturb Initial value of the randomization term (i.e. omega).
#' @param C0 Threshold of pre-test on instrument strength (default to 10).
#'
#' @return A group lasso object, which is a list of:
#' \item{data_part}{The S matrix in equation.}
#' \item{loglike}{Goodness of fit of data_part.}
#' \item{penalty}{Lambda term; the weights of penalty.}
#' \item{randomizer_scale}{Standard deviation of random noise.}
#' \item{Y}{The outcome.}
#' \item{D}{The treatment.}
#' \item{Z}{The p instruments.}
#' @export
#'
#' @importFrom expm sqrtm
#' @importFrom stats glm
#' @importFrom stats sd
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{group_lasso_iv} that fits a similar group lasso object. It
#' initializes the group lasso model with instrumental variables for an
#' TSLS test statistic.
#'
#' \link[stats]{glm} for fitting generalized linear models.
#'
#' @examples
#' # Initialize the group_lasso_iv_ar object with other term set to default
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' group_lasso_iv_ar(Y, D, Z)
group_lasso_iv_ar <- function(Y,
                              D,
                              Z,
                              penalty=NULL,
                              ridge_term=0,
                              randomizer_scale=NULL,
                              perturb=NULL,
                              C0=10) {

  n = nrow(Z)
  p = ncol(Z)
  ZTZ_inv = solve(t(Z) %*% Z)
  sqrtQ = expm::sqrtm(ZTZ_inv)
  ZTD = t(Z) %*% D
  data_part = sqrtQ %*% ZTD

  #loglike = glm(data_part ~ diag(p), family = "gaussian")
  loglike = glm(data_part ~ cbind(rep(0,p), diag(p)), family = "gaussian")
  nfeature = p

  # penalty
  if (is.null(penalty)) {
    penalty = sqrt((t(D) %*% D - t(ZTD) %*% ZTZ_inv %*% ZTD) * p * C0 / (n-p))
  }

  mean_diag = 1 # X is identity matrix here
  if (p > 1) {
    if (is.null(randomizer_scale)) {
      randomizer_scale = sqrt(mean_diag) * 0.5 * sd(data_part) *
        sqrt((length(data_part)-1)/length(data_part)) * sqrt(n / (n - 1))
    } else {
      randomizer_scale = randomizer_scale *
        (sqrt(mean_diag) * sd(data_part * sqrt(n / (n - 1))))
    }
  } else if (is.null(randomizer_scale)) {
    randomizer_scale = 0.5
  }

  return_list <- list("data_part" = data_part,
                      "loglike" = loglike,
                      "penalty" = penalty[1],
                      "randomizer_scale" = randomizer_scale,
                      "Y" = Y,
                      "D" = D,
                      "Z" = Z)
  return(return_list)
}


#' Fit Group Lasso Model with IV for Anderson-Rubin Test Statistic
#'
#' Establish and obtain results from the penalized convex optimization problem
#' for the Anderson-Rubin test statistic.
#'
#' @param gl_ar The group lasso object for AR test statistic representing the
#'              penalized convex optimization equation (generated by
#'              group_lasso_iv function)
#' @param perturb Initial value of the randomization term (i.e. omega)
#'
#' @return A fitted two-stage least-squares object, which is a list:
#' \item{initial_omega}{Initial randomization noise}
#' \item{initial_soln}{Initial solution from the optimization problem(i.e. vector v)}
#' \item{observed_opt_state}{Initial state for opt variables (2-norm of vector v)}
#' \item{observed_score_state}{The negated S matrix}
#' \item{initial_subgrad}{Initial L2 regularization part after scaling}
#' \item{lagrange}{Lambda term; the weights of penalty}
#' \item{active_directions}{Active directions of vector v (same value as opt_linear)}
#' \item{opt_linear}{Linear part of the optimization problem}
#' \item{opt_offset}{Offset part of the optimization problem}
#' \item{covariance}{Covariance of randomizer}
#' \item{precision}{Precision of randomizer}
#' \item{cond_precision}{Conditional precision}
#' \item{cond_cov}{Conditional covariance}
#' \item{logdens_linear}{Linear part of log density}
#' \item{cond_mean}{Conditional mean}
#' \item{log_density}{A function to calculate log density}
#' \item{affine_con}{The box constraint for affine Gaussian distribution}
#' \item{sampler}{The optimization sampler}
#' @export
#'
#' @importFrom MASS mvrnorm
#' @importFrom CVXR Variable
#' @importFrom CVXR Minimize
#' @importFrom CVXR Problem
#' @importFrom CVXR sum_squares
#' @importFrom CVXR norm
#' @importFrom CVXR solve
#' @importFrom pryr partial
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{fit_tsls} that fits a similar model. It fits the group lasso model
#' with instrumental variables for an TSLS test statistic.
#'
#' \link[MASS]{mvrnorm} for sampling from multivariate normal distribution.
#'
#' \link[CVXR]{Variable}, \link[CVXR]{Minimize}, \link[CVXR]{Problem},
#' \link[CVXR]{sum_squares}, \link[CVXR]{norm}, \link[CVXR]{solve} for
#' constructing and solving an optimization problem.
#'
#' \link{constraints} for creating the box constraint.
#'
#' \link{affine_gaussian_sampler} for creating a sampler to generate
#' samples from an affine truncated Gaussian distribution.
#'
#' @examples
#' # Fitting the model without specifying perturb (i.e. initial_omega)
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl_ar <- group_lasso_iv_ar(Y, D, Z)
#' fit_ar(gl_ar)
#'
#' # Fitting the model specifying perturb (i.e. initial_omega)
#' initial_omega = t(matrix(c(1.51039801, 0.34261832, 0.83800456)))
#' fit_ar(gl_ar, perturb = initial_omega)
fit_ar <- function(#Y,
                   #D,
                   #Z,
                   gl_ar,
                   perturb=NULL) {

  Y = gl_ar$Y
  D = gl_ar$D
  Z = gl_ar$Z

  n = nrow(Z)
  p = ncol(Z)

  # Generate randomization term (omega)
  if (is.null(perturb) == FALSE) {
    initial_omega = perturb
  } else {
    # initial_omega = randomizer.sample()
    noise_mean = rep(0, p)
    noise_Sigma = gl_ar$randomizer_scale^2 * diag(p)
    initial_omega = MASS::mvrnorm(1, noise_mean, noise_Sigma)
    initial_omega = t(matrix(initial_omega))
  }

  # Solve the penalized convex optimization program with CVXR
  # Variables minimized over
  v <- CVXR::Variable(p)

  # Problem definition
  objective <- CVXR::Minimize(1/2 * CVXR::sum_squares(v - gl_ar$data_part) +
                                gl_ar$penalty * CVXR::norm(v, "2") -
                                initial_omega %*% v)
  prob <- CVXR::Problem(objective)

  # Problem solution
  CVXR_solution <- CVXR::solve(prob)
  #CVXR_solution$status

  # initial solution for v
  initial_soln = CVXR_solution$getValue(v)
  initial_scalings = norm(initial_soln, "2")
  observed_opt_state = initial_scalings

  # initial state for opt variables
  initial_subgrad = -(initial_soln - gl_ar$data_part - t(initial_omega))

  num_opt_var = 1
  X = diag(p)
  y = gl_ar$data_part
  observed_score_state = -y
  lagrange = gl_ar$penalty
  active_directions = initial_subgrad / lagrange

  # form linear part
  opt_linear_term = active_directions
  opt_linear = opt_linear_term

  # form offset part
  opt_offset = initial_subgrad

  # compute implied mean and covariance
  cov = gl_ar$randomizer_scale^2
  prec = 1/gl_ar$randomizer_scale^2

  cond_precision = t(opt_linear_term) %*% opt_linear_term * prec
  cond_cov = solve(cond_precision)
  logdens_linear = cond_cov %*% t(opt_linear_term) * prec

  cond_mean = -logdens_linear %*% (observed_score_state + initial_subgrad)

  new_log_density = pryr::partial(log_density,
                                  logdens_linear=logdens_linear,
                                  offset=opt_offset,
                                  cond_prec=cond_precision,
                                  active_directions=active_directions,
                                  lagrange=lagrange,
                                  Z=Z)

  # now make the constraints
  A_scaling = -diag(num_opt_var)
  b_scaling = rep(0, num_opt_var)
  affine_con <- conditionalInference::constraints(A_scaling,
                                                  b_scaling,
                                                  mean=cond_mean,
                                                  covariance=cond_cov)

  sampler = conditionalInference::affine_gaussian_sampler(affine_con = affine_con,
                                                          initial_point = observed_opt_state,
                                                          observed_score_state = observed_score_state,
                                                          log_density = new_log_density,
                                                          logdens_linear = logdens_linear,
                                                          opt_offset = opt_offset)

  return_list <- list("initial_omega" = initial_omega,
                      "initial_soln" = initial_soln,
                      "observed_opt_state" = observed_opt_state,
                      "observed_score_state" = observed_score_state,
                      "initial_subgrad" = initial_subgrad,
                      "lagrange" = lagrange,
                      "active_directions" = active_directions,
                      "opt_linear" = opt_linear,
                      "opt_offset" = opt_offset,
                      "covariance" = cov,
                      "precision" = prec,
                      "cond_precision" = cond_precision,
                      "cond_cov" = cond_cov,
                      "logdens_linear" = logdens_linear,
                      "cond_mean" = cond_mean[1],
                      "log_density" = new_log_density,
                      "affine_con" = affine_con,
                      "sampler" = sampler)
  return(return_list)
}


#' Summary of Anderson-Rubin Test Statistic
#'
#' Give summary statistics of hypothesis testing, including pivots, p-values,
#' and confidence intervals/powers/coverage of TSLS test statistics.
#'
#' @param gl_ar The group lasso object for AR test statistic representing the
#'              penalized convex optimization equation (generated by
#'              group_lasso_iv function).
#' @param opt_sampler Sampler to generate opt samples.
#' @param parameter Put beta_0 for null hypothesis. Default to 0.
#' @param Sigma_11 Variance (i.e. estimate for sigma_11). Default to 1.
#' @param Sigma_12 Covariance (i.e. estimate for sigma_12). Default to 0.8.
#' @param level Significance level of F-test. Default to 0.95 (0 <= level <= 1).
#' @param ndraw Number of draws/samples after burnin. Default to 1000.
#' @param burnin Number of burnin samples. Default to 1000.
#' @param compute_intervals Binary indicator of whether to compute confidence intervals.
#'                          Default to FALSE.
#' @param compute_power Binary indicator of whether to compute power. Default to FALSE.
#' @param beta_alternative A sequence of alternative beta candidates to compute
#'                         confidence interval/coverage. Default to NULL.
#'
#' @details
#' Pivot quantities are generated based on the user-specific null hypothesis
#' \eqn{H_0: \beta^* = \beta_0}, where \eqn{\beta_0} can be specified through
#' the argument the argument ‘parameter.’ If the ‘parameter’ argument
#' is not specified, we test the default null hypothesis \eqn{H_0: \beta^* = \beta_0}.
#'
#' If all observed target statistics are tested against the null hypothesis
#' \eqn{H_0: \beta^* = 0}, then p-values will be the same as pivot quantities.
#' Otherwise, p-values are computed separately under the null hypothesis
#' \eqn{H_0: \beta^* = 0} instead of the user-specified hypothesis \eqn{H_0: \beta^* = \beta_0},
#' where \eqn{\beta_0 \neq 0}.
#'
#' Selective confidence intervals are constructed for the observed target if
#' the user set the argument ‘compute_intervals’ to be TRUE. This argument
#' is default to FALSE.
#'
#' Investigators can also compute powers of the hypothesis test by setting the ‘compute_power’
#' argument to TRUE (default to FALSE) and not setting the ‘compute_intervals’ argument to TRUE
#' (see the function ‘confidence_interval_coverage()’ for more details). In other words,
#' we want to compute the probability that the test correctly rejects H0 when the given
#' H1 is true. We want to know the true positive detection rate conditional on the actual
#' existence of an treatment effect.
#'
#' @return A two-stage least-squares summary object, which is a list of:
#' \item{observed_target}{Initial AR test statistic}
#' \item{cov_target}{Covariance of AR test statistic}
#' \item{cov_target_score}{Covariance of AR score}
#' \item{two_stage_ls}{TSLS estimator as beta reference for confidence interval}
#' \item{observed_target_tsls}{Initial TSLS test statistic}
#' \item{K1}{ZTY matrix of AR test statistic}
#' \item{K2}{ZTD matrix of AR test statistic}
#' \item{opt_sample}{Opt samples generated from the box constraint}
#' \item{pivots}{Pivotal quantities of AR test statistic}
#' \item{pvalues}{Simulated p-values from truncated normal distribution of AR test statistic}
#' \item{intervals (if applicable)}{Confidence intervals of AR test statistic}
#' \item{coverage (if applicable)}{Coverage of AR test statistic}
#' \item{powers (if applicable}{Powers of AR test statistic}
#' @export
#'
#' @importFrom expm sqrtm
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{summary_tsls} that returns the summary statistics for group lasso
#' optimization problem with instrumental variables for an TSLS test statistic.
#'
#' \link{sample_from_constraints} for using Gibbs sampler to simulate
#' from the box constraint.
#'
#' \link{coefficient_pvalues_iv_ar} for constructing selective
#' p-values for AR statistic.
#'
#' \link{confidence_intervals_iv_ar} for constructing selective confidence
#' intervals for AR statistic.
#'
#' \link{confidence_interval_coverage} for testing selective confidence
#' intervals for AR statistic.
#'
#' @examples
#' # Return summary statistics for AR test statistic
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl_ar <- group_lasso_iv_ar(Y, D, Z)
#' model_ar <- fit_ar(gl_ar)
#'
#' # Compute confidence intervals
#' summary_ar(gl_ar, model_ar$sampler, compute_intervals=TRUE)
#' # Compute power
#' summary_ar(gl_ar, model_ar$sampler, compute_power=TRUE)
#' # Compute neither confidence intervals nor power
#' summary_ar(gl_ar, model_ar$sampler)
summary_ar <- function(gl_ar,
                       opt_sampler,
                       parameter=0,
                       Sigma_11=1,
                       Sigma_12=0.8,
                       level=0.95,
                       ndraw=1000,
                       burnin=1000,
                       compute_intervals=FALSE,
                       compute_power=FALSE,
                       beta_alternative=NULL) {

  # initialize parameters
  Y = gl_ar$Y
  D = gl_ar$D
  Z = gl_ar$Z
  data_part = gl_ar$data_part

  # this is for pivot -- could use true beta^*
  # if (is.null(parameter)) {
  #   parameter = 0
  # }

  # compute the observed_target for AR statistic
  # target = Z^T (Y-D \beta_0)
  ZTZ = t(Z) %*% Z
  ZTZ_inv = solve(ZTZ)
  ZTD = t(Z) %*% D
  ZTY = t(Z) %*% Y
  DPZD = t(ZTD) %*% ZTZ_inv %*% ZTD
  DPZY = t(ZTD) %*% ZTZ_inv %*% ZTY

  K1 = ZTY
  K2 = ZTD
  observed_target = K1 - K2 * parameter

  # compute cov_target, cov_target_score
  cov_target = ZTZ * Sigma_11
  cov_target_score = expm::sqrtm(ZTZ_inv)
  cov_target_score = cov_target_score * (-Sigma_12)

  # tsls estimator as beta reference for confidence interval
  two_stage_ls = DPZY / DPZD
  observed_target_tsls = K1 - K2 %*% two_stage_ls

  # this is for Anderson-Rubin
  alternatives = rep('greater', 1)

  opt_sample = sample_from_constraints(con=opt_sampler$affine_con,
                                       Y=opt_sampler$initial_point,
                                       ndraw=ndraw,
                                       burnin=burnin)
  #print(opt_sample)

  # Generate partial test_stat() function
  # new_test_stat = pryr::partial(test_stat,
  #                               Y=Y,
  #                               D=D,
  #                               Z=Z)

  pivots = coefficient_pvalues_iv_ar(observed_target=observed_target,
                                     target_cov=cov_target,
                                     score_cov=cov_target_score,
                                     linear_func=K2,
                                     gl_ar=gl_ar,
                                     opt_sampler=opt_sampler,
                                     parameter=parameter,
                                     sample=opt_sample,
                                     alternatives=alternatives)

  if (!all(parameter == 0)) {
    parameter = as.matrix(parameter)
    pivots = coefficient_pvalues_iv_ar(observed_target=observed_target,
                                       target_cov=cov_target,
                                       score_cov=cov_target_score,
                                       linear_func=K2,
                                       gl_ar=gl_ar,
                                       opt_sampler=opt_sampler,
                                       parameter=matrix(0, nrow(parameter), ncol(parameter)),
                                       sample=opt_sample,
                                       alternatives=alternatives)
  } else {
    pvalues = pivots
  }

  intervals = NULL

  coverage = confidence_interval_coverage(observed_target=observed_target_tsls,
                                          target_cov=cov_target,
                                          score_cov=cov_target_score,
                                          linear_func=K2,
                                          gl_ar=gl_ar,
                                          opt_sampler=opt_sampler,
                                          beta_reference=two_stage_ls[1,1],
                                          beta_candidate=parameter,
                                          sample=opt_sample,
                                          level=level)

  if (compute_intervals) {
    intervals = confidence_intervals_iv_ar(observed_target=observed_target_tsls,
                                           target_cov=cov_target,
                                           score_cov=cov_target_score,
                                           linear_func=K2,
                                           gl_ar=gl_ar,
                                           opt_sampler=opt_sampler,
                                           beta_reference=two_stage_ls[1,1],
                                           sample=opt_sample,
                                           level=level,
                                           how_many_sd=4000)

    return_list <- list("observed_target" = observed_target,
                        "cov_target" = cov_target,
                        "cov_target_score" = cov_target_score,
                        "two_stage_ls" = two_stage_ls,
                        "observed_target_tsls" = observed_target_tsls,
                        "K1" = K1,
                        "K2" = K2,
                        "opt_sample" = opt_sample,
                        "pivots" = pivots,
                        "pvalues" = pvalues,
                        "intervals" = intervals)
    return(return_list)
  }

  if (compute_power) {
    if (is.null(beta_alternative)) {
      beta_alternative = c(-5:-1, 1:5) * 0.02 + parameter[1]
    }

    pidx = 1
    for (beta in beta_alternative) {
      detection = confidence_interval_coverage(observed_target_tsls,
                                               cov_target,
                                               cov_target_score,
                                               K2,
                                               gl_ar,
                                               opt_sampler,
                                               beta_reference=two_stage_ls[1,1],
                                               beta_candidate=beta,
                                               sample=opt_sample,
                                               level=level)
      if (pidx == 1) {
        powers = c(!detection)
      } else {
        powers = c(powers, !detection)
      }
      pidx = pidx + 1
    }

    return_list <- list("observed_target" = observed_target,
                        "cov_target" = cov_target,
                        "cov_target_score" = cov_target_score,
                        "two_stage_ls" = two_stage_ls,
                        "observed_target_tsls" = observed_target_tsls,
                        "K1" = K1,
                        "K2" = K2,
                        "opt_sample" = opt_sample,
                        "pivots" = pivots,
                        "pvalues" = pvalues,
                        "coverage" = coverage,
                        "powers" = powers)
    return(return_list)
  }

  return_list <- list("observed_target" = observed_target,
                      "cov_target" = cov_target,
                      "cov_target_score" = cov_target_score,
                      "two_stage_ls" = two_stage_ls,
                      "observed_target_tsls" = observed_target_tsls,
                      "K1" = K1,
                      "K2" = K2,
                      "opt_sample" = opt_sample,
                      "pivots" = pivots,
                      "pvalues" = pvalues,
                      "coverage" = coverage)
  return(return_list)
}


#' Test Statistic
#'
#' Return the result by comparing parameter of interest and observed
#' target test statistic.
#'
#' @param Y The outcome
#' @param D The treatment
#' @param Z The p instruments
#' @param parameter Parameter of interest
#' @param target Observed target test statistic
#'
#' @return The result matrix of the test.
#' @export
#'
#' @importFrom MASS ginv
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @examples
#' # Fit the group lasso optimization model for AR statisitc
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl_ar <- group_lasso_iv_ar(Y, D, Z)
#' model_ar <- fit_ar(gl_ar)
#'
#' # Estimate covariance and obtain summary statistics
#' cov = estimate_covariance(Y, D, Z)
#' s <- summary_ar(gl_ar, model_ar$sampler,
#'                 Sigma_11=cov[1,1], Sigma_12=cov[1,2])
#'
#' # Construct the optimization intervals before computing confidence intervals
#' # for AR test statistic
#' opt_sampling_info = list(sets::tuple(model_ar$sampler,
#'                                      s$opt_sample,
#'                                      s$cov_target,
#'                                      s$cov_target_score))
#' intervals <- optimization_intervals(opt_sampling_info,
#'                                     s$observed_target,
#'                                     nrow(s$opt_sample))
#'
#' # Construct the observed statistic using the test_stat method
#' observed_stat <- test_stat(gl_ar$Y,
#'                            gl_ar$D,
#'                            gl_ar$Z,
#'                            s$two_stage_ls[1,1],
#'                            intervals$observed - s$K2*0)
#' observed_stat
#'
#' # Construct the sample statistic using the test_stat method
#' sample_stat <- test_stat(gl_ar$Y,
#'                          gl_ar$D,
#'                          gl_ar$Z,
#'                          s$two_stage_ls[1,1],
#'                          intervals$normal_sample)
#' sample_stat
#'
#' # We can then compare the sample statistic and observed statistic,
#' # and using the weighted result to calculate the pivot quantity
#' weights <- c(0.09295276, 0.14619586, 0.29340068, 0.09858847, 0.22387328,
#'              0.36631718, 1.00000000, 0.48428178, 0.11871560, 0.25548842,
#'              0.07810723, 0.04390910, 0.16664036, 0.34837691, 0.08737997,
#'              0.09005764, 0.23332698, 0.14983265, 0.08651515, 0.12559373)
#' comp = apply(sample_stat, c(1,2), function(x) x <= observed_stat)
#' pivot = mean(comp * weights) / mean(weights)
test_stat <- function(Y,
                      D,
                      Z,
                      parameter,
                      target) {

  n = nrow(Z)
  p = ncol(Z)

  # target is of n by p
  # if (nrow(target) != n || ncol(target) != p) {
  #   target = t(target)
  # }
  if (ncol(target) != p) {
    target = t(target)
  }

  n = nrow(Z)
  p = ncol(Z)
  P_Z = Z %*% MASS::ginv(Z)
  R_Z = diag(n) - P_Z
  denom = t(Y-D*parameter) %*% R_Z %*% (Y-D*parameter) / (n-p)
  ZTZ_inv = solve(t(Z) %*% Z)
  result = rowSums(target * as.vector(t(ZTZ_inv %*% t(target))))
  result = as.matrix(result)
  result = result / p / denom[1,1]

  return(result)
}


#' Naive Inference for AR Test Statistic
#'
#' Give a simple inference of AR test statistic that does not take
#' instrument selection/strength test into account.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#' @param pass_pre_test Binary indicator of whether passing the pre-test.
#' @param parameter Put beta_0 for null hypothesis.
#' @param Sigma_11 Variance (i.e. estimate for sigma_11).
#' @param Sigma_12 Covariance (i.e. estimate for sigma_12).
#' @param Sigma_22 Variance (i.e. estimate for sigma_22).
#' @param compute_intervals Binary indicator of whether to compute confidence intervals.
#' @param level Significance level of F-test.
#'
#' @return A naive inference for AR test statistic, which includes components:
#' \item{ar}{An AR estimate for the treatment effect.}
#' \item{var_11}{An estimate of variance of AR test statistic.}
#' \item{var_22}{An estimate of variance of AR test statistic.}
#' \item{pval}{The p-value of AR test statistic.}
#' \item{interval}{The confidence interval of AR test statistic.}
#' @export
#'
#' @importFrom stats pf
#' @importFrom stats qf
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{naive_inference_tsls} that performs a naive inference for TSLS statistic.
#'
#' \link[stats]{pf} for the distribution function of the F distribution.
#'
#' \link[stats]{qf} for the quantile function of the F distribution.
#'
#' @examples
#' # Naive inference for AR test statistic with confidence intervals
#' # given passing the pre-test
#' b <- bigaussian_instance(n=10, p=5, s=2, snr=3)
#' naive_inference_ar(Y=b$Y,
#'                    D=b$D,
#'                    Z=b$Z,
#'                    pass_pre_test=TRUE,
#'                    compute_intervals=TRUE)
#'
#' # Naive inference for AR test statistic without confidence intervals
#' # given passing the pre-test
#' naive_inference_ar(Y=b$Y, D=b$D, Z=b$Z, pass_pre_test=TRUE)
#'
#' # Naive inference for AR test statistic given not passing the pre-test
#' naive_inference_ar(Y=b$Y, D=b$D, Z=b$Z, pass_pre_test=FALSE)
naive_inference_ar <- function(Y,
                               D,
                               Z,
                               pass_pre_test,
                               parameter=NULL,
                               Sigma_11=1,
                               Sigma_12=0.8,
                               Sigma_22=1,
                               compute_intervals=FALSE,
                               level=0.95) {

  if (is.null(parameter)) {
    parameter = 0
  }

  if (pass_pre_test) {
    ZTD = t(Z) %*% D
    ZTY = t(Z) %*% Y
    target = ZTY - ZTD * parameter

    n = nrow(Z)
    p = ncol(Z)

    P_Z = Z %*% MASS::ginv(Z)
    R_Z = diag(n) - P_Z
    denom = t(Y-D*parameter) %*% R_Z %*% (Y-D*parameter) / (n-p)
    ZTZ_inv = solve(t(Z) %*% Z)
    tar = t(target) %*% ZTZ_inv %*% target / p / denom

    # calculate p-value
    pval = 1 - pf(tar, df1 = p, df2 = n-p)

    # define F critical values
    upper_crit <- 1/qf((1-level)/2, p, n-p)
    lower_crit <- qf(level/2, n-p, p)

    # define variances
    var1 <- sqrt(Sigma_11 / denom)
    var2 <- sqrt(Sigma_22 / denom)

    # compute interval (if applicable)
    interval = NULL
    if (compute_intervals) {
      interval = c(tar - (var1/var2) * lower_crit, tar + (var1/var2) * upper_crit)
    }
  } else {
    tar = NULL
    var1 = NULL
    var2 = NULL
    pval = NULL
    interval = NULL
  }

  return_list <- list("ar" = tar,
                      "var_11" = var1,
                      "var_22" = var2,
                      "pval" = pval,
                      "interval" = interval)
  return(return_list)
}


