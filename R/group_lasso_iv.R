#' Group Lasso Model with Instrumental Variables
#'
#' Initialize outcome, treatment, and instrument matrices as well as
#' other terms for penalized convex optimization program.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#' @param penalty Lambda term in the optimization model.
#' @param ridge_term L2 regularization term (if applicable). Default to 0.
#' @param randomizer_scale Standard deviation of random noise distribution.
#' @param perturb Initial value of the randomization term (i.e. omega).
#' @param C0 Threshold of pre-test on instrument strength. Default to 10.
#'
#' @details
#' This function sets up the penalized convex optimization model:
#'
#' \eqn{\hat{v} = \underset{v}{\operatorname{argmin}} \frac{1}{2}||v-S||_2^2 + \lambda||v||_2 - \omega^Tv}
#'
#' where S is the data_part, \eqn{\lambda} is the weight of penalty, and \eqn{\omega}
#' is a random noise from a known Normal distribution.
#'
#' @return A group lasso object, which is a list of:
#' \item{data_part}{The S matrix in equation.}
#' \item{loglike}{Goodness of fit of data_part. Regress data_part on a zeros
#' vector that represent the intercept and an identity matrix that represents
#' the independent variables.}
#' \item{penalty}{Lambda term; the weights of penalty. If penalty is not specified,
#' the method calculates the penalty term based on the equation:
#' \eqn{\lambda = \sqrt{C_0 \frac{p}{n-p}\left(\sum_{i=1}^{n}(D_i-Z_i^T\hat{\gamma})^2\right)}}}
#' \item{randomizer_scale}{Standard deviation of random noise.}
#' \item{Y}{The outcome.}
#' \item{D}{The treatment.}
#' \item{Z}{The p instruments.}
#' @export
#'
#' @importFrom expm sqrtm
#' @importFrom stats glm
#' @importFrom stats sd
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{group_lasso_iv_ar} that fits a similar group lasso object. It
#' initializes the group lasso model with instrumental variables for an
#' AR test statistic.
#'
#' \link[stats]{glm} for fitting generalized linear models
#'
#' @examples
#' # Initialize the group_lasso_iv object with other term set to default
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' group_lasso_iv(Y, D, Z)
#'
#' # Initalize the group_lasso_iv object specifying other terms
#' group_lasso_iv(Y, D, Z, C0=10)
#' group_lasso_iv(Y, D, Z, penalty=5, randomizer_scale=0.8, C0=10)
group_lasso_iv <- function(Y,
                           D,
                           Z,
                           penalty=NULL,
                           ridge_term=0,
                           randomizer_scale=NULL,
                           perturb=NULL,
                           C0=10) {

  n = nrow(Z)
  p = ncol(Z)
  ZTZ_inv = solve(t(Z) %*% Z)
  sqrtQ = expm::sqrtm(ZTZ_inv)
  ZTD = t(Z) %*% D
  data_part = sqrtQ %*% ZTD  # S matrix

  #loglike = glm(data_part ~ diag(p), family = "gaussian")
  loglike = glm(data_part ~ cbind(rep(0,p), diag(p)), family = "gaussian")
  nfeature = p

  # C_0 is taken to be 10 for now
  # if (is.null(C0)) {
  #   C0 = 10
  # }

  # penalty
  if (is.null(penalty)) {
    penalty = sqrt((t(D) %*% D - t(ZTD) %*% ZTZ_inv %*% ZTD) * C0 * p / (n-p))
  }

  # ridge_term
  # if (is.null(ridge_term)) {
  #   ridge_term = 0
  # }

  mean_diag = 1 # X is identity matrix here
  if (p > 1) {
    if (is.null(randomizer_scale)) {
      randomizer_scale = sqrt(mean_diag) * 0.5 * sd(data_part) *
        sqrt((length(data_part)-1)/length(data_part)) * sqrt(n / (n - 1))
    } else {
      randomizer_scale = randomizer_scale *
        (sqrt(mean_diag) * sd(data_part * sqrt(n / (n - 1))))
    }
  } else if (is.null(randomizer_scale)) {
    randomizer_scale = 0.5
  }

  return_list <- list("data_part" = data_part,
                      "loglike" = loglike,
                      "penalty" = penalty[1],
                      "randomizer_scale" = randomizer_scale,
                      "Y" = Y,
                      "D" = D,
                      "Z" = Z)
  return(return_list)
}


#' Fit Group Lasso Model with IV for TSLS Test Statistic
#'
#' Establish and obtain results from the penalized convex optimization problem
#' for Two Stage Least Squares test statistic.
#'
#' @param gl The group lasso object representing the penalized convex
#'           optimization equation (generated by group_lasso_iv function).
#' @param perturb Initial value of the randomization term (i.e. omega).
#'
#' @details
#' Use optimization conditions and change-of-variables to derive tractable
#' conditional probability of TSLS test statistic. The conditional null
#' density of TSLS test statistic is computed based on
#'
#' \eqn{\ell_{\beta_0}(S,\omega | d>0, u) = f_{\beta_0}(S) * g(du - S + \lambda u) *
#' \mathbb{I}(d>0) * |\mathcal{J}|, \hspace{0.5cm} |\mathcal{J}| = (d+\lambda)^{(p-1)}}
#'
#' where \eqn{f_{\beta_0}(S)} is the original null density of \eqn{S} matrix under \eqn{H_0}.
#' \eqn{g(du-S+\lambda u)=g(\omega)} is the distribution of the random noise,
#' which is assumed to be Normal. \eqn{\mathbb{I}(d>0)} is asymptotically equivalent
#' to the conditioning event \{\eqn{F \geq C_0}\}. \eqn{\mathcal{J}} is the Jacobian term
#' from the change-of-variable formula.
#'
#' @return A fitted two-stage least-squares object, which is a list:
#' \item{initial_omega}{Initial randomization noise.}
#' \item{initial_soln}{Initial solution from the optimization problem (i.e. vector v).}
#' \item{observed_opt_state}{Initial solution for opt variables (i.e. 2-norm of vector v).}
#' \item{observed_score_state}{The negated S matrix. Recall that the S matrix
#' is the `data_part` attribute returned by the group_lasso_iv method.}
#' \item{initial_subgrad}{Initial state for optimization variables.}
#' \item{lagrange}{Lambda term; the weights of penalty.}
#' \item{active_directions}{Active directions of vector v (same value as opt_linear).}
#' \item{opt_linear}{Linear part of the optimization problem. It approximately
#' equals to vector u.}
#' \item{opt_offset}{Offset part of the optimization problem. It approximately
#' equals to the vecotr -(du - S - w)}
#' \item{covariance}{Covariance of randomizer.}
#' \item{precision}{Precision of randomizer.}
#' \item{cond_precision}{Conditional precision.}
#' \item{cond_cov}{Conditional covariance.}
#' \item{logdens_linear}{Linear part of log density.}
#' \item{cond_mean}{Conditional mean.}
#' \item{log_density}{A function to calculate log density.}
#' \item{affine_con}{The box constraint for affine Gaussian distribution.}
#' \item{sampler}{The optimization sampler.}
#' @export
#'
#' @importFrom MASS mvrnorm
#' @importFrom CVXR Variable
#' @importFrom CVXR Minimize
#' @importFrom CVXR Problem
#' @importFrom CVXR sum_squares
#' @importFrom CVXR norm
#' @importFrom CVXR solve
#' @importFrom pryr partial
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{fit_ar} that fits a similar model. It fits the group lasso model
#' with instrumental variables for an AR test statistic.
#'
#' \link[MASS]{mvrnorm} for sampling from multivariate normal distribution.
#'
#' \link[CVXR]{Variable}, \link[CVXR]{Minimize}, \link[CVXR]{Problem},
#' \link[CVXR]{sum_squares}, \link[CVXR]{norm}, \link[CVXR]{solve} for
#' constructing and solving an optimization problem.
#'
#' \link{constraints} for creating the box constraint.
#'
#' \link{affine_gaussian_sampler} for creating a sampler to generate samples
#' from an affine truncated Gaussian distribution.
#'
#' @examples
#' # Fitting the model without specifying perturb (i.e. initial_omega)
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl <- group_lasso_iv(Y, D, Z)
#' fit_tsls(gl)
#'
#' # Fitting the model specifying perturb (i.e. initial_omega)
#' initial_omega = t(matrix(c(1.51039801, 0.34261832, 0.83800456)))
#' fit_tsls(gl, perturb = initial_omega)
fit_tsls <- function(gl,
                     perturb=NULL) {

  Y = gl$Y
  D = gl$D
  Z = gl$Z

  n = nrow(Z)
  p = ncol(Z)

  # Generate randomization term (omega)
  if (is.null(perturb) == FALSE) {
    initial_omega = perturb
  } else {
    noise_mean = rep(0, p)
    noise_Sigma = (gl$randomizer_scale)^2 * diag(p)
    initial_omega = MASS::mvrnorm(1, noise_mean, noise_Sigma)
    initial_omega = t(matrix(initial_omega))
  }

  # Solve the penalized convex optimization program with CVXR
  # Variables minimized over
  v <- CVXR::Variable(p)

  # Problem definition
  objective <- CVXR::Minimize(1/2 * CVXR::sum_squares(v - gl$data_part) +
                          gl$penalty * CVXR::norm(v, "2") -
                          initial_omega %*% v)
  prob <- CVXR::Problem(objective)

  # Problem solution
  CVXR_solution <- CVXR::solve(prob)
  #CVXR_solution$status

  # initial solution for v
  initial_soln = CVXR_solution$getValue(v) # vector v_hat
  initial_scalings = norm(initial_soln, "2") # vector d
  observed_opt_state = initial_scalings # vector d

  # initial state for opt variables
  # lambda * u = -(du - S - w)
  initial_subgrad = -(initial_soln - gl$data_part - t(initial_omega))

  num_opt_var = 1
  X = diag(p)
  y = gl$data_part
  observed_score_state = -y
  lagrange = gl$penalty
  active_directions = initial_subgrad / lagrange # vector u

  # form linear part
  opt_linear_term = active_directions
  opt_linear = opt_linear_term # u

  # form offset part
  opt_offset = initial_subgrad # -(du - S - w)

  # compute implied mean and covariance
  cov = gl$randomizer_scale^2
  prec = 1/gl$randomizer_scale^2

  cond_precision = t(opt_linear_term) %*% opt_linear_term * prec
  cond_cov = solve(cond_precision)
  logdens_linear = cond_cov %*% t(opt_linear_term) * prec

  cond_mean = -logdens_linear %*% (observed_score_state + initial_subgrad)

  new_log_density = pryr::partial(log_density,
                                  logdens_linear=logdens_linear,
                                  offset=opt_offset,
                                  cond_prec=cond_precision,
                                  active_directions=active_directions,
                                  lagrange=lagrange,
                                  Z=Z)

  # now make the constraints
  A_scaling = -diag(num_opt_var)
  b_scaling = rep(0, num_opt_var)
  affine_con <- conditionalInference::constraints(A_scaling,
                                                  b_scaling,
                                                  mean=cond_mean,
                                                  covariance=cond_cov)

  sampler = conditionalInference::affine_gaussian_sampler(affine_con = affine_con,
                                    initial_point = observed_opt_state,
                                    observed_score_state = observed_score_state,
                                    log_density = new_log_density,
                                    logdens_linear = logdens_linear,
                                    opt_offset = opt_offset)

  return_list <- list("initial_omega" = initial_omega,
                      "initial_soln" = initial_soln,
                      "observed_opt_state" = observed_opt_state,
                      "observed_score_state" = observed_score_state,
                      "initial_subgrad" = initial_subgrad,
                      "lagrange" = lagrange,
                      "active_directions" = active_directions,
                      "opt_linear" = opt_linear,
                      "opt_offset" = opt_offset,
                      "covariance" = cov,
                      "precision" = prec,
                      "cond_precision" = cond_precision,
                      "cond_cov" = cond_cov,
                      "logdens_linear" = logdens_linear,
                      "cond_mean" = cond_mean[1],
                      "log_density" = new_log_density,
                      "affine_con" = affine_con,
                      "sampler" = sampler)
  return(return_list)
}


#' Log density
#'
#' Calculate the log density of given equation. It includes the Jacobian term.
#'
#' @param logdens_linear Linear part of log density
#' @param offset Offset part of log density
#' @param cond_prec Conditional precision
#' @param score Score state for opt variables
#' @param opt State for opt variables
#' @param active_directions Active directions of vector v (same as opt_linear)
#' @param lagrange Lambda term; the weights of penalty
#' @param Z The p instruments
#'
#' @return The log density of given equation.
#' @export
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{log_density_ray} for calculating the log density of normal sample.
#'
#' \link{log_jacobian} for calculating the log jacobian term of the given
#' problem.
#'
#' @examples
#' # Fit the group lasso optimization model
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl <- group_lasso_iv(Y, D, Z)
#' model <- fit_tsls(gl)
#'
#' # Estimate covariance and obtain summary statistics
#' cov = estimate_covariance(Y, D, Z)
#' s <- summary_tsls(gl, model$sampler,
#'                   Sigma_11 = cov[1,1], Sigma_12 = cov[1,2])
#'
#' # Estimate log density of current optimization samples (using the
#' # partial log_density function from optimization sampler)
#' model$sampler$log_density(model$sampler$observed_score_state, s$opt_sample)
#'
#' # Estimate log density of current optimization samples (directly using the
#' # log_density function)
#' log_density(logdens_linear=model$logdens_linear,
#'             offset=model$opt_offset,
#'             cond_prec=model$cond_precision,
#'             score=model$sampler$observed_score_state,
#'             opt=s$opt_sample,
#'             active_directions=model$active_directions,
#'             lagrange=model$lagrange,
#'             Z=Z)
log_density <- function(logdens_linear,
                        offset,
                        cond_prec,
                        score,
                        opt,
                        active_directions,
                        lagrange,
                        Z) {

  if (length(dim(score)) == 1 || dim(score)[2] == 1) {
    mean_term = t(logdens_linear %*% (score + offset))
  } else {
    term = score + array(offset, dim = dim(score))
    mean_term = t(logdens_linear %*% t(array(term, dim = dim(logdens_linear))))
  }

  arg = apply(opt, c(1,2), function(x) x + mean_term)
  jacobian_part = conditionalInference::log_jacobian(Z, opt, active_directions, lagrange)

  temp = apply(t(arg), c(1,2), function (x) x*cond_prec)
  return (-0.5 * rowSums(arg * t(temp)) + jacobian_part)

}


#' Log Jacobian
#'
#' Calculate the log jacobian term of the given problem. The computed
#' log jacobian part is used to calculate the log density of
#' optimization samples.
#'
#' @param Z The p instruments
#' @param opt State for opt variables
#' @param active_directions Active directions of vector v (same as opt_linear)
#' @param lagrange Lambda term; the weights of penalty
#'
#' @return The log jacobian term.
#' @export
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{log_density} for calculating the log density of given samples.
#' To calculate the log density, a log jacobian part is needed, which
#' is computed by this function.
#'
#' @examples
#' # Fit the group lasso optimization model
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl <- group_lasso_iv(Y, D, Z)
#' model <- fit_tsls(gl)
#'
#' # Estimate covariance and obtain summary statistics
#' cov = estimate_covariance(Y, D, Z)
#' s <- summary_tsls(gl, model$sampler,
#'                   Sigma_11 = cov[1,1], Sigma_12 = cov[1,2])
#'
#'
#' # Compute the log jacobian part of current optimization samples
#' log_jacobian(Z=Z,
#'              opt=s$opt_sample,
#'              active_directions=model$active_directions,
#'              lagrange=model$lagrange)
log_jacobian <- function(Z,
                         opt, #opt_sample
                         active_directions,
                         lagrange) {

  n = nrow(Z)
  p = ncol(Z)

  if (p == 1) {
    return(0)
  }

  V = matrix(0, p, p-1)

  # function null
  A = t(matrix(active_directions))
  svd_A = svd(A)
  d = svd_A$d
  u = svd_A$u
  v = svd_A$v

  eps=1e-12
  padding = max(0, ncol(A) - length(d))
  null_mask = c(d <= eps, rep(TRUE, padding))

  null_space = c()
  for (i in 1:length(v)) {
    if (null_mask[i]) {
      null_space = c(null_space, v[i])
    }
  }
  V = t(matrix(null_space))

  component = lagrange * (t(V) %*% V)
  jacobs = array()
  for (item in opt) {
    jacobs = append(jacobs, det(item * diag(p-1) + component))
  }
  jacobs = jacobs[2:(n+1)]
  #jacobs = det(item * diag(p-1) + component)

  return(log(jacobs))
}


#' Summary of Two-Stage Least Squares
#'
#' Give summary statistics of hypothesis testing, including pivots, p-values,
#' and confidence intervals of TSLS test statistics.
#'
#' @param gl The group lasso object representing the penalized convex
#'           optimization equation (generated by group_lasso_iv function).
#' @param opt_sampler Sampler to generate opt samples.
#' @param parameter Put beta_0 for null hypothesis. Default to 0.
#' @param Sigma_11 Variance (i.e. estimate for sigma_11). Default to 1.
#' @param Sigma_12 Covariance (i.e. estimate for sigma_12). Default to 0.8.
#' @param level Significance level of F-test. Default to 0.95 (0 <= level <= 1).
#' @param ndraw Number of draws/samples after burnin. Default to 1000.
#' @param burnin Number of burnin samples. Default to 1000.
#' @param compute_intervals Binary indicator of whether to compute confidence intervals.
#'                          Default to TRUE.
#'
#' @details
#' Pivot quantities are generated based on the user-specific null hypothesis
#' \eqn{H_0: \beta^* = \beta_0}, where \eqn{\beta_0} can be specified through
#' the argument the argument ‘parameter.’ If the ‘parameter’ argument
#' is not specified, we test the default null hypothesis \eqn{H_0: \beta^* = \beta_0}.
#'
#' If all observed target statistics are tested against the null hypothesis
#' \eqn{H_0: \beta^* = 0}, then p-values will be the same as pivot quantities.
#' Otherwise, p-values are computed separately under the null hypothesis
#' \eqn{H_0: \beta^* = 0} instead of the user-specified hypothesis \eqn{H_0: \beta^* = \beta_0},
#' where \eqn{\beta_0 \neq 0}.
#'
#' Selective confidence intervals are constructed for the observed target if
#' the user set the argument ‘compute_intervals’ to be TRUE. This argument
#' is default to TRUE.
#'
#' @return A two-stage least-squares summary object, which is a list of:
#' \item{observed_target}{Initial TSLS test statistic.}
#' \item{cov_target}{Covariance of TSLS.}
#' \item{cov_target_score}{Covariance of TSLS score.}
#' \item{opt_sample}{Opt samples generated from the box constraint.}
#' \item{pivots}{Pivotal quantities of TSLS test statistic.}
#' \item{pvalues}{Simulated p-values from truncated normal distribution of TSLS test statistic.}
#' \item{intervals}{Confidence intervals of TSLS test statistic.}
#' @export
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{summary_ar} that returns the summary statistics for group lasso
#' optimization problem with instrumental variables for an AR test statistic.
#'
#' \link{sample_from_constraints} that uses Gibbs sampler
#' to simulate from the box constraint.
#'
#' \link{coefficient_pvalues_iv} for constructing selective
#' p-values for TSLS statistic.
#'
#' \link{confidence_intervals_iv} for constructing selective
#' confidence intervals for TSLS statistic.
#'
#' @examples
#' # Fit the group lasso optimization model
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' gl <- group_lasso_iv(Y, D, Z)
#' model <- fit_tsls(gl)
#'
#' # Return summary statistics for TSLS test statistic
#' summary_tsls(gl, model$sampler)
summary_tsls <- function(gl,
                         opt_sampler,
                         parameter=0,
                         Sigma_11=1,
                         Sigma_12=0.8,
                         level=0.95,
                         ndraw=1000,
                         burnin=1000,
                         compute_intervals=TRUE) {

  # initialize parameters
  Y = gl$Y
  D = gl$D
  Z = gl$Z
  data_part = gl$data_part

  n = nrow(Z)
  p = ncol(Z)

  # if (is.null(parameter)) {
  #   parameter = 0
  # }

  # compute tsls, i.e. the observed target
  ZTD = t(Z) %*% D
  ZTY = t(Z) %*% Y
  ZTZ_inv = solve(t(Z) %*% Z)
  DPZD = t(ZTD) %*% ZTZ_inv %*% ZTD
  DPZY = t(ZTD) %*% ZTZ_inv %*% ZTY
  two_stage_ls = DPZY / DPZD

  observed_target = two_stage_ls
  #observed_opt_state = initial_point = initial_scalings

  # compute cov_target, cov_target_score
  X = diag(p)
  y = data_part

  cov_target = Sigma_11/DPZD
  cov_target_score = -((Sigma_12/DPZD)[1,1])*y

  alternatives = rep('twosided', 1)

  opt_sample = sample_from_constraints(con=opt_sampler$affine_con,
                                       Y=opt_sampler$initial_point,
                                       ndraw=ndraw,
                                       burnin=burnin)

  pivots = coefficient_pvalues_iv(observed_target=observed_target,
                                  target_cov=cov_target,
                                  score_cov=cov_target_score,
                                  opt_sampler=opt_sampler,
                                  parameter=parameter,
                                  sample=opt_sample,
                                  alternatives=alternatives)

  if (!all(parameter == 0)) {
    parameter = as.matrix(parameter)
    pvalues = coefficient_pvalues_iv(observed_target=observed_target,
                                    target_cov=cov_target,
                                    score_cov=cov_target_score,
                                    opt_sampler=opt_sampler,
                                    parameter=matrix(0, nrow(parameter), ncol(parameter)),
                                    sample=opt_sample,
                                    alternatives=alternatives)
  } else {
    pvalues = pivots
  }

  intervals = NULL
  if (compute_intervals) {
    intervals = confidence_intervals_iv(observed_target,
                                       cov_target,
                                       cov_target_score,
                                       opt_sampler=opt_sampler,
                                       sample=opt_sample,
                                       level=level)
  }

  return_list <- list("observed_target" = observed_target,
                      "cov_target" = cov_target,
                      "cov_target_score" = cov_target_score,
                      "opt_sample" = opt_sample,
                      "pivots" = pivots,
                      "pvalues" = pvalues,
                      "intervals" = intervals)
  return(return_list)
}


#' Estimate Covariance
#'
#' Compute the covariance matrix of TSLS or AR test statistic.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#'
#' @return The estimated 2x2 covariance matrix.
#' @export
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @examples
#' # Estimate the covariance
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'            Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' estimate_covariance(Y, D, Z)
estimate_covariance <- function(Y,
                                D,
                                Z) {

  n = nrow(Z)

  ZTD = t(Z) %*% D
  ZTY = t(Z) %*% Y
  ZTZ_inv = solve(t(Z) %*% Z)
  YPZY = t(ZTY) %*% ZTZ_inv %*% ZTY
  DPZD = t(ZTD) %*% ZTZ_inv %*% ZTD
  DPZY = t(ZTD) %*% ZTZ_inv %*% ZTY
  YRY = t(Y) %*% Y - YPZY
  DRY = t(D) %*% Y - DPZY
  DRD = t(D) %*% D - DPZD
  two_stage_ls = DPZY / DPZD
  cov_estim_00 = (YRY + (two_stage_ls^2) * DRD - 2 * two_stage_ls * DRY)/n
  cov_estim_01 = (DRY - two_stage_ls * DRD)/n
  cov_estim_10 = (DRY - two_stage_ls * DRD)/n
  cov_estim_11 = (DRD)/n

  cov_estim = matrix(c(cov_estim_00, cov_estim_01, cov_estim_10, cov_estim_11), 2, 2)
  sigma_11 = cov_estim_00; sigma_12 = cov_estim_01

  return(cov_estim)
}


#' Naive Inference for TSLS Test Statistic
#'
#' Give a simple inference of TSLS test statistic that does not take
#' instrument selection/strength test into account.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#' @param pass_pre_test Binary indicator of whether passing the pre-test.
#' @param parameter Put beta_0 for null hypothesis.
#' @param Sigma_11 Variance (i.e. estimate for sigma_11).
#' @param compute_intervals Binary indicator of whether to compute confidence intervals.
#' @param level Significance level of F-test.
#'
#' @return A naive inference for TSLS test statistic, which includes components:
#' \item{tsls}{An TSLS estimate for the treatment effect.}
#' \item{std}{An estimate of standard error of TSLS test statistic.}
#' \item{pval}{The p-value of TSLS test statistic.}
#' \item{interval}{The confidence interval of TSLS test statistic.}
#' @export
#'
#' @importFrom stats pnorm
#' @importFrom stats qnorm
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link{naive_inference_ar} that performs a naive inference for AR statistic.
#'
#' \link[stats]{pnorm} for the distribution function of normal distribution.
#'
#' \link[stats]{qnorm} for the quantile function of normal distribution.
#'
#' @examples
#' # Naive inference for TSLS test statistic with confidence intervals
#' # given passing the pre-test
#' b <- bigaussian_instance(n=10, p=5, s=2, snr=3)
#' naive_inference_tsls(Y=b$Y,
#'                      D=b$D,
#'                      Z=b$Z,
#'                      pass_pre_test=TRUE,
#'                      compute_intervals=TRUE)
#'
#' # Naive inference for TSLS test statistic without confidence intervals
#' # given passing the pre-test
#' naive_inference_tsls(Y=b$Y, D=b$D, Z=b$Z, pass_pre_test=TRUE)
#'
#' # Naive inference for TSLS test statistic given not passing the pre-test
#' naive_inference_tsls(Y=b$Y, D=b$D, Z=b$Z, pass_pre_test=FALSE)
naive_inference_tsls <- function(Y,
                                 D,
                                 Z,
                                 pass_pre_test,
                                 parameter=NULL,
                                 Sigma_11=1,
                                 compute_intervals=FALSE,
                                 level=0.95) {

  if (is.null(parameter)) {
    parameter = 0
  }

  if (pass_pre_test) {
    ZTD = t(Z) %*% D
    ZTY = t(Z) %*% Y
    ZTZ_inv = solve(t(Z) %*% Z)

    denom = t(ZTD) %*% ZTZ_inv %*% ZTD
    tsls = (t(ZTD) %*% ZTZ_inv %*%  ZTY) / denom
    std = sqrt(Sigma_11 / denom)

    pval = pnorm(tsls, mean = parameter, sd = std)
    pval = 2 * min(pval, 1-pval)

    interval = NULL
    if (compute_intervals) {
      interval = c(tsls - std * qnorm(p=(level+1)/2), tsls + std * qnorm(p=(level+1)/2))
    }
  } else {
    tsls = NULL
    std = NULL
    pval = NULL
    interval = NULL
  }

  return_list <- list("tsls" = tsls,
                      "std" = std,
                      "pval" = pval,
                      "interval" = interval)
  return(return_list)
}


#' Bigaussian Instance
#'
#' Generate a dataset, including the outcome, treatment, and instrument matrices,
#' for simulation. The dataset can be used to test the instrument strength and
#' the treatment effect.
#'
#' @param n Number of samples.
#' @param p Number of instruments.
#' @param s Number of invalid instruments.
#' @param snr Coefficient of invalid instruments.
#' @param gsnr True gamma parameter.
#' @param beta True beta parameter.
#' @param Sigma Noise variance-covariance matrix.
#' @param rho Z matrix structure.
#' @param scale Binary indicator of whether it is a weak IV case.
#' @param center Binary indicator of whether re-center the distribution to 0.
#'
#' @return A bigaussian instance, which is a list containing
#' \item{Y}{The outcome.}
#' \item{D}{The treatment.}
#' \item{Z}{The p instruments.}
#' \item{beta}{True beta parameter.}
#' \item{gamma}{A vector of of length p of true gamma parameter.}
#' @export
#'
#' @importFrom MASS mvrnorm
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @seealso
#' \link[MASS]{mvrnorm} for sampling from multivariate normal distribution.
#'
#' @examples
#' # True beta value is 1
#' bigaussian_instance(n=10, p=5, s=2, snr=3, gsnr=1, beta=1,
#' Sigma = rbind(c(1, 0.8), c(0.8, 1)), rho=0)
#'
#' # True beta value is 0
#' bigaussian_instance(n=100, p=10, s=7, snr=3, gsnr=1, beta=0,
#' Sigma = rbind(c(1, 0.8), c(0.8, 1)), rho=0)
bigaussian_instance <- function(n=1000,
                                p=10,
                                s=3,
                                snr=7,
                                gsnr=1,
                                beta=1,
                                Sigma=rbind(c(1, 0.8), c(0.8, 1)),
                                rho=0,
                                scale=FALSE,
                                center=TRUE) {

  # Generate parameters
  # --> gamma coefficient
  gamma = rep(gsnr, p)

  # Generate samples
  # Generate Z matrix
  mat = sqrt(rho) * matrix(rnorm(n), nrow = n, ncol = 1)
  Z = (sqrt(1-rho) * matrix(rnorm(n*p), nrow = n, ncol = p) +
         matrix(replicate(p, mat), nrow = n))

  # center to 0
  if (center) {
    Z = Z - colMeans(Z)
  }

  # scale for weak IV case
  if (scale) {
    Z = Z / (apply(Z, 2, sd) * sqrt(n))
  }

  # Generate error term
  mean = rep(0,2)
  errorTerm = MASS::mvrnorm(n, mean, Sigma)

  # Generate D and Y
  D = Z %*% gamma + errorTerm[1:n, 2]
  Y = D * beta + errorTerm[1:n, 1]

  D = D - mean(D)
  Y = Y - mean(Y)

  return_list <- list("Z" = Z, "D" = D, "Y" = Y, "beta" = beta, "gamma" = gamma)
  return(return_list)
}


#' Instrument Strength Test
#'
#' Simulate a pre-test on instrument strength before inferring the treatment
#' effect. This method use a F-test to test the instrument strength.
#'
#' @param Y The outcome.
#' @param D The treatment.
#' @param Z The p instruments.
#' @param C0 Threshold of the pre-test.
#'
#' @details
#' The pre-test regresses the treatment D on the instruments Z to to assess
#' the instrument strength. The F-test is conducted with the null hypothesis
#' \eqn{H_0: \gamma^*=0} that there is no association between the treatment and
#' instruments, so instruments are irrelevant. The alternative hypothesis
#' \eqn{H_1:\gamma^*\neq 0} implies that there is an association between the treatment
#' and instruments, so instruments are relevant.
#'
#' @return TRUE if the pre-test is passed (F-statistic exceeds the threshold)
#'         and FALSE otherwise.
#' @export
#'
#' @references
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2019). Inference After Selecting
#' Plausibly Valid Instruments with Application to Mendelian Randomization.
#'
#' Bi, Nan & Kang, Hyunseung & Taylor, Jonathan. (2020). Inferring Treatment
#' Effects After Testing Instrument Strength in Linear Models.
#'
#' @examples
#' # Perform a pre-test on instrument strength
#' Z = matrix(rnorm(10*3), nrow = 10, ncol = 3) +
#'     matrix(replicate(3, matrix(0, nrow = 10, ncol = 1)),
#'     nrow = 10)
#' errorTerm = MASS::mvrnorm(n=10, mu=rep(0,2),
#'             Sigma=rbind(c(1, 0.8),c(0.8, 1)))
#' D = Z %*% rep(1, 3) + errorTerm[1:10, 2]
#' Y = D * 1 + errorTerm[1:10, 1]
#' pre_test(Y, D, Z)
pre_test <- function(Y,
                     D,
                     Z,
                     C0=NULL) {

  n = nrow(Z)
  p = ncol(Z)

  ZTD = t(Z) %*% D
  ZTY = t(Z) %*% Y
  ZTZ_inv = solve(t(Z) %*% Z)
  DPZD = t(ZTD) %*% ZTZ_inv %*% ZTD
  DPZY = t(ZTD) %*% ZTZ_inv %*% ZTY
  denom = (t(D) %*% D - DPZD) / (n-p)
  num = DPZD / p
  f = num / denom
  ftest = f

  # C0 is taken to be 10 for now
  # C0 = 10
  if (is.null(C0)) {
    C0 = 10
  }

  result <- NULL
  if (f >= C0) {
    result <- TRUE
  } else {
    result <- FALSE
  }
  return(result)
}
